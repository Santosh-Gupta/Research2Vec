{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Research2VecPublicPlayGround.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeL2pLE0FN7W"
      },
      "source": [
        "UPDATE 9-20-18\n",
        "\n",
        "I have a new version, which so far looks to be giving much better results. Link to new version. \n",
        "\n",
        "https://github.com/Santosh-Gupta/Research2Vec/blob/master/Research2VecPublicPlayGroundVersion2.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgJ4ofMW_vfU"
      },
      "source": [
        "↓↓↓  Stuff you don't have to touch (unless you want to) just run it ↓↓↓ This will download the Tensorflow Model (2 GB) and necessary Python dictionaries into your Colab environment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9CP0M3XT2Db_",
        "outputId": "f453a807-9545-478b-a3c6-24753b8688ca"
      },
      "source": [
        "!pip3 install --upgrade --force-reinstall tensorflow-gpu==1.15.0 \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/72/d06017379ad4760dc58781c765376ce4ba5dcf3c08d37032eeefbccf1c51/tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 33kB/s \n",
            "\u001b[?25hCollecting six>=1.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 38.9MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.8MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 50.2MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/4e/de63de3cd9a83d3c1753a4566b11fc9d90b845f2448a132cfd36d3cb3cd1/protobuf-3.15.8-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 52.3MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
            "Collecting wheel>=0.26\n",
            "  Downloading https://files.pythonhosted.org/packages/65/63/39d04c74222770ed1589c0eaba06c05891801219272420b40311cd60c880/wheel-0.36.2-py2.py3-none-any.whl\n",
            "Collecting google-pasta>=0.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[?25hCollecting absl-py>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/c9/ef0fae29182d7a867d203f0eff8296b60da92098cc41db33a434f4be84bf/absl_py-0.12.0-py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 54.3MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.8.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/d8/1bfe90cc49c166dd2ec1be46fa4830c254ce702004a110830c74ec1df0c0/grpcio-1.37.1-cp37-cp37m-manylinux2014_x86_64.whl (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 30.1MB/s \n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
            "Collecting opt-einsum>=2.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.6MB/s \n",
            "\u001b[?25hCollecting astor>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
            "Collecting keras-preprocessing>=1.0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.9MB/s \n",
            "\u001b[?25hCollecting numpy<2.0,>=1.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/ef/8967d406f3f85018ceb5efab50431e901683188f1741ceb053efcab26c87/numpy-1.20.2-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 35.9MB/s \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 39.2MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/33/1ae0f71395e618d6140fbbc9587cc3156591f748226075e0f7d6f9176522/Markdown-3.3.4-py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.8MB/s \n",
            "\u001b[?25hCollecting setuptools>=41.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/42/2876a3a136f8bfa9bd703518441c8db78ff1eeaddf174baa85c083c1fd15/setuptools-56.0.0-py3-none-any.whl (784kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 45.6MB/s \n",
            "\u001b[?25hCollecting h5py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/c5/94e2444eb691f658fb8e3cf6cde3ae29540cf6d9ce76f0561afcdbb89136/h5py-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 37.3MB/s \n",
            "\u001b[?25hCollecting importlib-metadata; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/e2/49966924c93909d47612bb47d911448140a2f6c1390aec2f4c1afbe3748f/importlib_metadata-4.0.1-py3-none-any.whl\n",
            "Collecting cached-property; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
            "Collecting typing-extensions>=3.6.4; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/35/6c4fff5ab443b57116cb1aad46421fb719bed2825664e8fe77d66d99bcbc/typing_extensions-3.10.0.0-py3-none-any.whl\n",
            "Collecting zipp>=0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/8c/715c54e9e34c0c4820f616a913a7de3337d0cd79074dd1bed4dd840f16ae/zipp-3.4.1-py3-none-any.whl\n",
            "Building wheels for collected packages: gast, wrapt, termcolor\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=23ef4ba9247c0c2c9d127d761f95569797c404d1d22161af71dd7bed14d0e21b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68665 sha256=16c9a1da9ac9737730753e1d3f533736a53711c2ac61217a05edc5240ef10b4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c2/ed/d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-cp37-none-any.whl size=4832 sha256=a17a32321a87d11d4983dceec87dfc2570eeb05555b736ba1dc8ac2f1f0214f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
            "Successfully built gast wrapt termcolor\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement grpcio~=1.32.0, but you'll have grpcio 1.37.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement h5py~=2.10.0, but you'll have h5py 3.2.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.20.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement typing-extensions~=3.7.4, but you'll have typing-extensions 3.10.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, numpy, werkzeug, grpcio, protobuf, absl-py, wheel, typing-extensions, zipp, importlib-metadata, markdown, setuptools, tensorboard, cached-property, h5py, keras-applications, gast, tensorflow-estimator, wrapt, google-pasta, termcolor, opt-einsum, astor, keras-preprocessing, tensorflow-gpu\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Found existing installation: grpcio 1.32.0\n",
            "    Uninstalling grpcio-1.32.0:\n",
            "      Successfully uninstalled grpcio-1.32.0\n",
            "  Found existing installation: protobuf 3.12.4\n",
            "    Uninstalling protobuf-3.12.4:\n",
            "      Successfully uninstalled protobuf-3.12.4\n",
            "  Found existing installation: absl-py 0.12.0\n",
            "    Uninstalling absl-py-0.12.0:\n",
            "      Successfully uninstalled absl-py-0.12.0\n",
            "  Found existing installation: wheel 0.36.2\n",
            "    Uninstalling wheel-0.36.2:\n",
            "      Successfully uninstalled wheel-0.36.2\n",
            "  Found existing installation: typing-extensions 3.7.4.3\n",
            "    Uninstalling typing-extensions-3.7.4.3:\n",
            "      Successfully uninstalled typing-extensions-3.7.4.3\n",
            "  Found existing installation: zipp 3.4.1\n",
            "    Uninstalling zipp-3.4.1:\n",
            "      Successfully uninstalled zipp-3.4.1\n",
            "  Found existing installation: importlib-metadata 3.10.1\n",
            "    Uninstalling importlib-metadata-3.10.1:\n",
            "      Successfully uninstalled importlib-metadata-3.10.1\n",
            "  Found existing installation: Markdown 3.3.4\n",
            "    Uninstalling Markdown-3.3.4:\n",
            "      Successfully uninstalled Markdown-3.3.4\n",
            "  Found existing installation: setuptools 56.0.0\n",
            "    Uninstalling setuptools-56.0.0:\n",
            "      Successfully uninstalled setuptools-56.0.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Found existing installation: termcolor 1.1.0\n",
            "    Uninstalling termcolor-1.1.0:\n",
            "      Successfully uninstalled termcolor-1.1.0\n",
            "  Found existing installation: opt-einsum 3.3.0\n",
            "    Uninstalling opt-einsum-3.3.0:\n",
            "      Successfully uninstalled opt-einsum-3.3.0\n",
            "  Found existing installation: astor 0.8.1\n",
            "    Uninstalling astor-0.8.1:\n",
            "      Successfully uninstalled astor-0.8.1\n",
            "  Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "Successfully installed absl-py-0.12.0 astor-0.8.1 cached-property-1.5.2 gast-0.2.2 google-pasta-0.2.0 grpcio-1.37.1 h5py-3.2.1 importlib-metadata-4.0.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.20.2 opt-einsum-3.3.0 protobuf-3.15.8 setuptools-56.0.0 six-1.15.0 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0 termcolor-1.1.0 typing-extensions-3.10.0.0 werkzeug-1.0.1 wheel-0.36.2 wrapt-1.12.1 zipp-3.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "absl",
                  "astor",
                  "gast",
                  "google",
                  "h5py",
                  "keras_preprocessing",
                  "numpy",
                  "opt_einsum",
                  "pkg_resources",
                  "six",
                  "tensorboard",
                  "tensorflow",
                  "termcolor",
                  "typing_extensions",
                  "wrapt"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HJm4Tiz27mr",
        "outputId": "4e5b0183-097a-4bf8-d6e3-4b417aa35504"
      },
      "source": [
        "!pip install keras==2.1.5"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/65/e4aff762b8696ec0626a6654b1e73b396fcc8b7cc6b98d78a1bc53b85b48/Keras-2.1.5-py2.py3-none-any.whl (334kB)\n",
            "\r\u001b[K     |█                               | 10kB 10.8MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 9.7MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 12.4MB/s eta 0:00:01\r\u001b[K     |████                            | 40kB 14.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 51kB 17.2MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 61kB 18.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 71kB 20.2MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81kB 21.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 92kB 22.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 102kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 112kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 122kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 133kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 143kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 153kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 163kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 174kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 184kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 194kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 204kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 215kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 225kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 235kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 245kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 256kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 266kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 276kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 286kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 296kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 307kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 317kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 327kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 337kB 18.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.20.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.15.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AerXiTE5_jD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee40089e-765a-409f-ffbc-7cdebfcce259"
      },
      "source": [
        "#SECTION 1\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "import time\n",
        "%matplotlib inline\n",
        "from __future__ import print_function\n",
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "from matplotlib import pyplot\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from sklearn.manifold import TSNE\n",
        "from urllib.request import urlretrieve\n",
        "from os.path import isfile, isdir\n",
        "import keras\n",
        "import requests\n",
        "import pickle\n",
        "!pip install https://github.com/Phlya/adjustText/archive/master.zip\n",
        "import importlib\n",
        "import adjustText\n",
        "importlib.reload(adjustText)\n",
        "from adjustText import adjust_text\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "                \n",
        "download_file_from_google_drive('15_inqIvXBnaC5mI_GPxLnV8Va8hxIqAM', 'model.zip') #using cbow 2 inputs\n",
        "\n",
        "if not os.path.exists('MODEL'):\n",
        "    os.makedirs('MODEL')\n",
        "\n",
        "# UNZIP ZIP\n",
        "print(\"Uncompressing zip file\")\n",
        "zip_ref = zipfile.ZipFile('model.zip', 'r')\n",
        "zip_ref.extractall('MODEL/')\n",
        "zip_ref.close()\n",
        "\n",
        "print( os.getcwd() )\n",
        "print( os.listdir('./MODEL') )\n",
        "\n",
        "download_file_from_google_drive('1mdF0JfrzbOxeHD26JaUW8KEfIdMEHzQG', 'titleDict.pickle')\n",
        "\n",
        "with open('titleDict.pickle', 'rb') as handle:\n",
        "    titleDict = pickle.load(handle)\n",
        "    \n",
        "download_file_from_google_drive('12fiWctMsUJbmq0JaT3H8Zh23GdtxZThh', 'idDict.pickle')\n",
        "\n",
        "with open('idDict.pickle', 'rb') as handle:\n",
        "    idDict = pickle.load(handle)\n",
        "    \n",
        "download_file_from_google_drive('1xOF1qw6B0Sl63ROovlBDU4DbfaJbplxY', 'linkDict.pickle')\n",
        "\n",
        "with open('linkDict.pickle', 'rb') as handle:\n",
        "    linkDict = pickle.load(handle)\n",
        "    \n",
        "with tf.Session() as sess:\n",
        "  saver = tf.train.import_meta_graph('./MODEL/Research2VecEmbedSize80.ckpt.meta')\n",
        "  saver.restore(sess, './MODEL/Research2VecEmbedSize80.ckpt' )\n",
        "\n",
        "  embeddings = tf.get_default_graph().get_tensor_by_name('embeddings:0') \n",
        "  softmax_weights = tf.get_default_graph().get_tensor_by_name('softmax_weights:0') \n",
        "  \n",
        "  normSM = tf.sqrt(tf.reduce_sum(tf.square(softmax_weights), 1, keepdims=True))\n",
        "  normalized_embeddingsSM = softmax_weights / normSM\n",
        "  \n",
        "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
        "  normalized_embeddings = embeddings / norm\n",
        "  \n",
        "  final_embeddings = normalized_embeddings.eval()\n",
        "  final_embeddingsSM = normalized_embeddingsSM.eval()\n",
        "  \n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/Phlya/adjustText/archive/master.zip\n",
            "  Using cached https://github.com/Phlya/adjustText/archive/master.zip\n",
            "Requirement already satisfied (use --upgrade to upgrade): adjustText==0.8b2 from https://github.com/Phlya/adjustText/archive/master.zip in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from adjustText==0.8b2) (1.20.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from adjustText==0.8b2) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->adjustText==0.8b2) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->adjustText==0.8b2) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->adjustText==0.8b2) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->adjustText==0.8b2) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->adjustText==0.8b2) (1.15.0)\n",
            "Building wheels for collected packages: adjustText\n",
            "  Building wheel for adjustText (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adjustText: filename=adjustText-0.8b2-cp37-none-any.whl size=9142 sha256=662f358a660aba25d2e3a22a999a1d3d3832f17d413422d9f0751f112c1e34bf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_8e_9lsc/wheels/7a/56/aa/ee86b18565ad149ba3a5470b55bbc5081701fd0b57141ed055\n",
            "Successfully built adjustText\n",
            "Uncompressing zip file\n",
            "/content\n",
            "['Research2VecEmbedSize80.ckpt.data-00000-of-00001', 'Research2VecEmbedSize80.ckpt.meta', 'checkpoint', 'Research2VecEmbedSize80.ckpt.index']\n",
            "INFO:tensorflow:Restoring parameters from ./MODEL/Research2VecEmbedSize80.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv-d9p1n3BLm"
      },
      "source": [
        "^^^ Stuff you don't have to touch (unless you want to) just run it ^^^"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBvRIA0onHPL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b327857a-ea5a-4632-8feb-c5c1715300ec"
      },
      "source": [
        "#Section 2\n",
        "\n",
        "#Use https://www.semanticscholar.org to first find papers you want to analyze, then 'titlePhrase' to find the paper in this recommender.  \n",
        "\n",
        "titlePhrase = 'autism detection'  \n",
        "lines = []\n",
        "for i in titleDict:\n",
        "  if type(titleDict[i]) is str:\n",
        "    if str.lower(titlePhrase) in str.lower(titleDict[i]) :\n",
        "        lines.append(i)\n",
        "print(\"Number of results \", len(lines))\n",
        "for j in lines:\n",
        "  print(\"EMBED ID:\", j, \"| TITLE:\", titleDict[j], \"| LINK:\",  linkDict[j] )\n",
        "  "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of results  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJkc32MX-5-d"
      },
      "source": [
        "#Section 3\n",
        "\n",
        "idString = '80fc283900e930a8ab5d7b525f258cf620b9bf8e'  \n",
        "lines = []\n",
        "for i in idDict:\n",
        "  if type(idDict[i]) is str:\n",
        "    if idString in str.lower(idDict[i]) :\n",
        "        lines.append(i)\n",
        "for j in lines:\n",
        "  print(idString)\n",
        "  print(\"EMBED ID:\", j, \"| TITLE:\", titleDict[j], \"| LINK:\",  linkDict[j] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlD9b8kVQ365"
      },
      "source": [
        "#Section 4\n",
        "\n",
        "NumberOfPapers = 100  #Number of most similar papers to view and analyze in TSNE\n",
        "\n",
        "#Set the embedID's of the paper(s) you want analyzed. \n",
        "paper1EmbedID = 1621681   #TITLE: Skip-Thought Vectors | LINK: https://semanticscholar.org/paper/d3bbd57899d938e8c4bcafbbda10ceb59638e4db\n",
        "paper2EmbedID = 431721 # TITLE: Convolutional Neural Networks for Sentence Classification | LINK: https://semanticscholar.org/paper/398dee13b3aaaefdf14c78cc1e00dcf265795fd3\n",
        "paper3EmbedID = 244279  #TITLE: Distributed Representations of Sentences and Documents | LINK: https://semanticscholar.org/paper/1510cf4b8abea80b9f352325ca4c132887de21a0\n",
        "paper4EmbedID = 1418824 # TITLE: Incremental sentence compression using LSTM recurrent networks | LINK: https://semanticscholar.org/paper/4720f79749d740e18bed4ade8d6b6b742c9dc112\n",
        "\n",
        "# ↓↓↓ You can adjust this to use any number of combined papers ↓↓↓\n",
        "\n",
        "paper1 = np.take(final_embeddings, paper1EmbedID , axis=0)   \n",
        "paper2 = np.take(final_embeddings, paper2EmbedID , axis=0) \n",
        "paper3 = np.take(final_embeddings, paper3EmbedID , axis=0)   \n",
        "paper4 = np.take(final_embeddings, paper4EmbedID , axis=0) \n",
        "\n",
        "extracted_v = paper1#+ paper2 + paper3 + paper4  #Don't need to average since vectors are normalized \n",
        "\n",
        "# ^^^ You can adjust this to use any number of combined papers ^^^\n",
        "\n",
        "dotprods_v = np.matmul(extracted_v, np.transpose(final_embeddings))\n",
        "\n",
        "nearestPapers = (-dotprods_v).argsort()[0:NumberOfPapers]\n",
        "\n",
        "for k in range(0, NumberOfPapers):\n",
        "  print(\"EMBED ID:\", nearestPapers[k], \"| TITLE:\", titleDict[nearestPapers[k]], \"| LINK:\",  linkDict[nearestPapers[k]] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQZiZOJASA-A"
      },
      "source": [
        "#Section 4\n",
        "\n",
        "NumberOfPapers = 100  #Number of most similar papers to view and analyze in TSNE\n",
        "\n",
        "#Set the embedID's of the paper(s) you want analyzed. \n",
        "paper1EmbedID = 1621681   #TITLE: Skip-Thought Vectors | LINK: https://semanticscholar.org/paper/d3bbd57899d938e8c4bcafbbda10ceb59638e4db\n",
        "paper2EmbedID = 431721 # TITLE: Convolutional Neural Networks for Sentence Classification | LINK: https://semanticscholar.org/paper/398dee13b3aaaefdf14c78cc1e00dcf265795fd3\n",
        "paper3EmbedID = 244279  #TITLE: Distributed Representations of Sentences and Documents | LINK: https://semanticscholar.org/paper/1510cf4b8abea80b9f352325ca4c132887de21a0\n",
        "paper4EmbedID = 1418824 # TITLE: Incremental sentence compression using LSTM recurrent networks | LINK: https://semanticscholar.org/paper/4720f79749d740e18bed4ade8d6b6b742c9dc112\n",
        "\n",
        "# ↓↓↓ You can adjust this to use any number of combined papers ↓↓↓\n",
        "\n",
        "paper1 = np.take(final_embeddingsSM, paper1EmbedID , axis=0)   \n",
        "paper2 = np.take(final_embeddingsSM, paper2EmbedID , axis=0) \n",
        "paper3 = np.take(final_embeddingsSM, paper3EmbedID , axis=0)   \n",
        "paper4 = np.take(final_embeddingsSM, paper4EmbedID , axis=0) \n",
        "\n",
        "extracted_v = paper1 #+ paper2 + paper3 + paper4  #Don't need to average since vectors are normalized \n",
        "\n",
        "# ^^^ You can adjust this to use any number of combined papers ^^^\n",
        "\n",
        "dotprods_v = np.matmul(extracted_v, np.transpose(final_embeddingsSM))\n",
        "\n",
        "nearestPapers = (-dotprods_v).argsort()[0:NumberOfPapers]\n",
        "\n",
        "for k in range(0, NumberOfPapers):\n",
        "  print(\"EMBED ID:\", nearestPapers[k], \"| TITLE:\", titleDict[nearestPapers[k]], \"| LINK:\",  linkDict[nearestPapers[k]] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSEA9WvnNlIh"
      },
      "source": [
        "#Section 4\n",
        "\n",
        "NumberOfPapers = 100  #Number of most similar papers to view and analyze in TSNE\n",
        "\n",
        "#Set the embedID's of the paper(s) you want analyzed. \n",
        "paper1EmbedID = 1621681   #TITLE: Skip-Thought Vectors | LINK: https://semanticscholar.org/paper/d3bbd57899d938e8c4bcafbbda10ceb59638e4db\n",
        "paper2EmbedID = 431721 # TITLE: Convolutional Neural Networks for Sentence Classification | LINK: https://semanticscholar.org/paper/398dee13b3aaaefdf14c78cc1e00dcf265795fd3\n",
        "paper3EmbedID = 244279  #TITLE: Distributed Representations of Sentences and Documents | LINK: https://semanticscholar.org/paper/1510cf4b8abea80b9f352325ca4c132887de21a0\n",
        "paper4EmbedID = 1418824 # TITLE: Incremental sentence compression using LSTM recurrent networks | LINK: https://semanticscholar.org/paper/4720f79749d740e18bed4ade8d6b6b742c9dc112\n",
        "\n",
        "# ↓↓↓ You can adjust this to use any number of combined papers ↓↓↓\n",
        "\n",
        "paper1 = np.take(final_embeddingsSM, paper1EmbedID , axis=0)   \n",
        "paper2 = np.take(final_embeddingsSM, paper2EmbedID , axis=0) \n",
        "paper3 = np.take(final_embeddingsSM, paper3EmbedID , axis=0)   \n",
        "paper4 = np.take(final_embeddingsSM, paper4EmbedID , axis=0) \n",
        "\n",
        "extracted_v = paper1 #+ paper2 + paper3 + paper4  #Don't need to average since vectors are normalized \n",
        "\n",
        "# ^^^ You can adjust this to use any number of combined papers ^^^\n",
        "\n",
        "dotprods_v = np.matmul(extracted_v, np.transpose(final_embeddings))\n",
        "\n",
        "nearestPapers = (-dotprods_v).argsort()[0:NumberOfPapers]\n",
        "\n",
        "for k in range(0, NumberOfPapers):\n",
        "  print(\"EMBED ID:\", nearestPapers[k], \"| TITLE:\", titleDict[nearestPapers[k]], \"| LINK:\",  linkDict[nearestPapers[k]] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXnN52ayNqLA"
      },
      "source": [
        "#Section 4\n",
        "\n",
        "NumberOfPapers = 100  #Number of most similar papers to view and analyze in TSNE\n",
        "\n",
        "#Set the embedID's of the paper(s) you want analyzed. \n",
        "paper1EmbedID = 1621681   #TITLE: Skip-Thought Vectors | LINK: https://semanticscholar.org/paper/d3bbd57899d938e8c4bcafbbda10ceb59638e4db\n",
        "paper2EmbedID = 431721 # TITLE: Convolutional Neural Networks for Sentence Classification | LINK: https://semanticscholar.org/paper/398dee13b3aaaefdf14c78cc1e00dcf265795fd3\n",
        "paper3EmbedID = 244279  #TITLE: Distributed Representations of Sentences and Documents | LINK: https://semanticscholar.org/paper/1510cf4b8abea80b9f352325ca4c132887de21a0\n",
        "paper4EmbedID = 1418824 # TITLE: Incremental sentence compression using LSTM recurrent networks | LINK: https://semanticscholar.org/paper/4720f79749d740e18bed4ade8d6b6b742c9dc112\n",
        "\n",
        "# ↓↓↓ You can adjust this to use any number of combined papers ↓↓↓\n",
        "\n",
        "paper1 = np.take(final_embeddings, paper1EmbedID , axis=0)   \n",
        "paper2 = np.take(final_embeddings, paper2EmbedID , axis=0) \n",
        "paper3 = np.take(final_embeddings, paper3EmbedID , axis=0)   \n",
        "paper4 = np.take(final_embeddings, paper4EmbedID , axis=0) \n",
        "\n",
        "extracted_v = paper1 #+ paper2 + paper3 + paper4  #Don't need to average since vectors are normalized \n",
        "\n",
        "# ^^^ You can adjust this to use any number of combined papers ^^^\n",
        "\n",
        "dotprods_v = np.matmul(extracted_v, np.transpose(final_embeddingsSM))\n",
        "\n",
        "nearestPapers = (-dotprods_v).argsort()[0:NumberOfPapers]\n",
        "\n",
        "for k in range(0, NumberOfPapers):\n",
        "  print(\"EMBED ID:\", nearestPapers[k], \"| TITLE:\", titleDict[nearestPapers[k]], \"| LINK:\",  linkDict[nearestPapers[k]] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnQ9Y9FFza3G"
      },
      "source": [
        "#Section 5\n",
        "\n",
        "#Run this for a T-SNE map for the returned similar papers. Will take 15-18 minutes\n",
        "\n",
        "for_TSNE = np.take(final_embeddings, nearestPapers, axis=0) \n",
        "\n",
        "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
        "two_d_embeddings = tsne.fit_transform(for_TSNE)\n",
        "\n",
        "texts = []\n",
        "\n",
        "def plot(embeddings, labels):\n",
        "  assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
        "  pyplot.figure(figsize=(40, 40))  # in inches\n",
        "\n",
        "  for i, label in enumerate(labels):\n",
        "    x, y = embeddings[i,:]\n",
        "    pyplot.scatter(x, y)\n",
        "    texts.append(pyplot.text(x, y, label))\n",
        "  \n",
        "  adjust_text(texts , arrowprops=dict(arrowstyle='->', color='#8f1402' ) ) \n",
        "  \n",
        "  pyplot.show()\n",
        "\n",
        "\n",
        "papers = [titleDict[i] for i in nearestPapers]\n",
        "plot(two_d_embeddings, papers)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VV9HizfQ_ae"
      },
      "source": [
        "\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from google.colab import files\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from numpy import genfromtxt\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}